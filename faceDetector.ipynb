{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion                                             pixels     Usage\n",
       "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
       "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
       "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
       "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
       "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import some libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "df = pd.read_csv('../datasets/fer2013/fer2013/fer2013.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['70' '80' '82' ... '106' '109' '82']\n"
     ]
    }
   ],
   "source": [
    "# preprocessing of data by segregatting the training and test datas first\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "for index, row in df.iterrows():\n",
    "    k = row['pixels'].split(\" \")\n",
    "    if row['Usage'] == 'Training':\n",
    "        X_train.append(np.array(k))\n",
    "        y_train.append(row['emotion'])\n",
    "    elif row['Usage'] == 'PublicTest':\n",
    "        X_test.append(np.array(k))\n",
    "        y_test.append(row['emotion'])\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we convert the array elements from string to integer\n",
    "X_train = np.array(X_train, dtype = 'uint8')\n",
    "y_train = np.array(y_train, dtype = 'uint8')\n",
    "X_test = np.array(X_test, dtype = 'uint8')\n",
    "y_test = np.array(y_test, dtype = 'uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we oneHotencode it using the keras api to_categorical instead of oneHotencoding\n",
    "#oneHotencoding and to_categorical are the same thing its just the matter of preference\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train= to_categorical(y_train, num_classes=7)\n",
    "y_test = to_categorical(y_test, num_classes=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28709, 2304)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1e289291970>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we know that the X_train array contains 28709 rows with 2304 elements in each \n",
    "# so we also know this 2304 are pixels meaning the array should be converted to 48*48 which is 2304\n",
    "print(np.shape(X_train))\n",
    "X_train = X_train.reshape(X_train.shape[0], 48, 48, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 48, 48, 1)\n",
    "plt.imshow(X_train[0])\n",
    "# basically we create a 4d tensor using reshape so X_train would be a 4d tensor with 28704 rows and 48 width and 48 height of each image and 1 is the grey rgb \n",
    "# here 1 is the grayscale form.. gray scale form means the colors are in the gray shades form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some image augmentation to increase the number of images and make our model more robust\n",
    "from keras.preprocessing.image import ImageDataGenerator \n",
    "datagen = ImageDataGenerator( \n",
    "    rescale=1./255,\n",
    "    rotation_range = 10,\n",
    "    horizontal_flip = True,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    fill_mode = 'nearest')\n",
    "testgen = ImageDataGenerator(rescale=1./255)\n",
    "datagen.fit(X_train)\n",
    "batch_size = 64 \n",
    "train_flow = datagen.flow(X_train, y_train, batch_size=batch_size) \n",
    "test_flow = testgen.flow(X_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the layers to be added to the model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import concatenate\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from keras.regularizers import l1, l2\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create the model for our training:\n",
    "def FER_Model(input_shape=(48,48,1)):\n",
    "    # first input model\n",
    "    visible = Input(shape=input_shape, name='input')\n",
    "    num_classes = 7\n",
    "    \n",
    "    #the 1-st block \n",
    "    conv1_1 = Conv2D(64, kernel_size=3, activation='relu', padding='same', name = 'conv1_1')(visible)\n",
    "    conv1_1 = BatchNormalization()(conv1_1)\n",
    "    conv1_2 = Conv2D(64, kernel_size=3, activation='relu', padding='same', name = 'conv1_2')(conv1_1)\n",
    "    conv1_2 = BatchNormalization()(conv1_2)\n",
    "    pool1_1 = MaxPooling2D(pool_size=(2,2), name = 'pool1_1')(conv1_2)\n",
    "    drop1_1 = Dropout(0.3, name = 'drop1_1')(pool1_1)\n",
    "    \n",
    "    \n",
    "    #the 2-nd block\n",
    "    conv2_1 = Conv2D(128, kernel_size=3, activation='relu', padding='same', name = 'conv2_1')(drop1_1)\n",
    "    conv2_1 = BatchNormalization()(conv2_1)\n",
    "    conv2_2 = Conv2D(128, kernel_size=3, activation='relu', padding='same', name = 'conv2_2')(conv2_1)\n",
    "    conv2_2 = BatchNormalization()(conv2_2)\n",
    "    conv2_3 = Conv2D(128, kernel_size=3, activation='relu', padding='same', name = 'conv2_3')(conv2_2)\n",
    "    conv2_2 = BatchNormalization()(conv2_3)\n",
    "    pool2_1 = MaxPooling2D(pool_size=(2,2), name = 'pool2_1')(conv2_3)\n",
    "    drop2_1 = Dropout(0.3, name = 'drop2_1')(pool2_1)\n",
    "    \n",
    "    #the 3-rd block\n",
    "    conv3_1 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv3_1')(drop2_1)\n",
    "    conv3_1 = BatchNormalization()(conv3_1)\n",
    "    conv3_2 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv3_2')(conv3_1)\n",
    "    conv3_2 = BatchNormalization()(conv3_2)\n",
    "    conv3_3 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv3_3')(conv3_2)\n",
    "    conv3_3 = BatchNormalization()(conv3_3)\n",
    "    conv3_4 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv3_4')(conv3_3)\n",
    "    conv3_4 = BatchNormalization()(conv3_4)\n",
    "    pool3_1 = MaxPooling2D(pool_size=(2,2), name = 'pool3_1')(conv3_4)\n",
    "    drop3_1 = Dropout(0.3, name = 'drop3_1')(pool3_1)\n",
    "    \n",
    "    #the 4-th block\n",
    "    conv4_1 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv4_1')(drop3_1)\n",
    "    conv4_1 = BatchNormalization()(conv4_1)\n",
    "    conv4_2 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv4_2')(conv4_1)\n",
    "    conv4_2 = BatchNormalization()(conv4_2)\n",
    "    conv4_3 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv4_3')(conv4_2)\n",
    "    conv4_3 = BatchNormalization()(conv4_3)\n",
    "    conv4_4 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv4_4')(conv4_3)\n",
    "    conv4_4 = BatchNormalization()(conv4_4)\n",
    "    pool4_1 = MaxPooling2D(pool_size=(2,2), name = 'pool4_1')(conv4_4)\n",
    "    drop4_1 = Dropout(0.3, name = 'drop4_1')(pool4_1)\n",
    "    \n",
    "    #the 5-th block\n",
    "    conv5_1 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name = 'conv5_1')(drop4_1)\n",
    "    conv5_1 = BatchNormalization()(conv5_1)\n",
    "    conv5_2 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name = 'conv5_2')(conv5_1)\n",
    "    conv5_2 = BatchNormalization()(conv5_2)\n",
    "    conv5_3 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name = 'conv5_3')(conv5_2)\n",
    "    conv5_3 = BatchNormalization()(conv5_3)\n",
    "    conv5_4 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name = 'conv5_4')(conv5_3)\n",
    "    conv5_3 = BatchNormalization()(conv5_3)\n",
    "    pool5_1 = MaxPooling2D(pool_size=(2,2), name = 'pool5_1')(conv5_4)\n",
    "    drop5_1 = Dropout(0.3, name = 'drop5_1')(pool5_1)\n",
    "    \n",
    "    #Flatten and output\n",
    "    flatten = Flatten(name = 'flatten')(drop5_1)\n",
    "    ouput = Dense(num_classes, activation='softmax', name = 'output')(flatten)# create model \n",
    "    model = Model(inputs =visible, outputs = ouput)\n",
    "    \n",
    "    # summary layers\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 48, 48, 1)]       0         \n",
      "                                                                 \n",
      " conv1_1 (Conv2D)            (None, 48, 48, 64)        640       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 48, 48, 64)       256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv1_2 (Conv2D)            (None, 48, 48, 64)        36928     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 48, 48, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " pool1_1 (MaxPooling2D)      (None, 24, 24, 64)        0         \n",
      "                                                                 \n",
      " drop1_1 (Dropout)           (None, 24, 24, 64)        0         \n",
      "                                                                 \n",
      " conv2_1 (Conv2D)            (None, 24, 24, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 24, 24, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2_2 (Conv2D)            (None, 24, 24, 128)       147584    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 24, 24, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2_3 (Conv2D)            (None, 24, 24, 128)       147584    \n",
      "                                                                 \n",
      " pool2_1 (MaxPooling2D)      (None, 12, 12, 128)       0         \n",
      "                                                                 \n",
      " drop2_1 (Dropout)           (None, 12, 12, 128)       0         \n",
      "                                                                 \n",
      " conv3_1 (Conv2D)            (None, 12, 12, 256)       295168    \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 12, 12, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv3_2 (Conv2D)            (None, 12, 12, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 12, 12, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv3_3 (Conv2D)            (None, 12, 12, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 12, 12, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv3_4 (Conv2D)            (None, 12, 12, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 12, 12, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " pool3_1 (MaxPooling2D)      (None, 6, 6, 256)         0         \n",
      "                                                                 \n",
      " drop3_1 (Dropout)           (None, 6, 6, 256)         0         \n",
      "                                                                 \n",
      " conv4_1 (Conv2D)            (None, 6, 6, 256)         590080    \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 6, 6, 256)        1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv4_2 (Conv2D)            (None, 6, 6, 256)         590080    \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 6, 6, 256)        1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv4_3 (Conv2D)            (None, 6, 6, 256)         590080    \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 6, 6, 256)        1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv4_4 (Conv2D)            (None, 6, 6, 256)         590080    \n",
      "                                                                 \n",
      " batch_normalization_12 (Bat  (None, 6, 6, 256)        1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " pool4_1 (MaxPooling2D)      (None, 3, 3, 256)         0         \n",
      "                                                                 \n",
      " drop4_1 (Dropout)           (None, 3, 3, 256)         0         \n",
      "                                                                 \n",
      " conv5_1 (Conv2D)            (None, 3, 3, 512)         1180160   \n",
      "                                                                 \n",
      " batch_normalization_13 (Bat  (None, 3, 3, 512)        2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv5_2 (Conv2D)            (None, 3, 3, 512)         2359808   \n",
      "                                                                 \n",
      " batch_normalization_14 (Bat  (None, 3, 3, 512)        2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv5_3 (Conv2D)            (None, 3, 3, 512)         2359808   \n",
      "                                                                 \n",
      " batch_normalization_15 (Bat  (None, 3, 3, 512)        2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv5_4 (Conv2D)            (None, 3, 3, 512)         2359808   \n",
      "                                                                 \n",
      " pool5_1 (MaxPooling2D)      (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      " drop5_1 (Dropout)           (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 7)                 3591      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,111,367\n",
      "Trainable params: 13,103,431\n",
      "Non-trainable params: 7,936\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Archit\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = FER_Model()\n",
    "opt = Adam(lr=0.0001, decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "448/448 [==============================] - 73s 135ms/step - loss: 2.0137 - accuracy: 0.2146 - val_loss: 1.8430 - val_accuracy: 0.2466\n",
      "Epoch 2/100\n",
      "448/448 [==============================] - 57s 128ms/step - loss: 1.7883 - accuracy: 0.2520 - val_loss: 1.8039 - val_accuracy: 0.2552\n",
      "Epoch 3/100\n",
      "448/448 [==============================] - 57s 128ms/step - loss: 1.7591 - accuracy: 0.2718 - val_loss: 1.7423 - val_accuracy: 0.2934\n",
      "Epoch 4/100\n",
      "448/448 [==============================] - 58s 129ms/step - loss: 1.7059 - accuracy: 0.3113 - val_loss: 1.9012 - val_accuracy: 0.3006\n",
      "Epoch 5/100\n",
      "448/448 [==============================] - 58s 129ms/step - loss: 1.6334 - accuracy: 0.3541 - val_loss: 1.6497 - val_accuracy: 0.3856\n",
      "Epoch 6/100\n",
      "448/448 [==============================] - 58s 129ms/step - loss: 1.5664 - accuracy: 0.3864 - val_loss: 1.4933 - val_accuracy: 0.4333\n",
      "Epoch 7/100\n",
      "448/448 [==============================] - 58s 129ms/step - loss: 1.4930 - accuracy: 0.4235 - val_loss: 1.3671 - val_accuracy: 0.4748\n",
      "Epoch 8/100\n",
      "448/448 [==============================] - 58s 129ms/step - loss: 1.4166 - accuracy: 0.4554 - val_loss: 1.3299 - val_accuracy: 0.4815\n",
      "Epoch 9/100\n",
      "448/448 [==============================] - 58s 130ms/step - loss: 1.3487 - accuracy: 0.4801 - val_loss: 1.3222 - val_accuracy: 0.4946\n",
      "Epoch 10/100\n",
      "448/448 [==============================] - 58s 130ms/step - loss: 1.3052 - accuracy: 0.4983 - val_loss: 1.2660 - val_accuracy: 0.5110\n",
      "Epoch 11/100\n",
      "448/448 [==============================] - 58s 130ms/step - loss: 1.2572 - accuracy: 0.5181 - val_loss: 1.2348 - val_accuracy: 0.5436\n",
      "Epoch 12/100\n",
      "448/448 [==============================] - 58s 130ms/step - loss: 1.2268 - accuracy: 0.5339 - val_loss: 1.2212 - val_accuracy: 0.5219\n",
      "Epoch 13/100\n",
      "448/448 [==============================] - 58s 129ms/step - loss: 1.1827 - accuracy: 0.5524 - val_loss: 1.1962 - val_accuracy: 0.5442\n",
      "Epoch 14/100\n",
      "448/448 [==============================] - 58s 130ms/step - loss: 1.1604 - accuracy: 0.5599 - val_loss: 1.1887 - val_accuracy: 0.5514\n",
      "Epoch 15/100\n",
      "448/448 [==============================] - 58s 130ms/step - loss: 1.1320 - accuracy: 0.5698 - val_loss: 1.1193 - val_accuracy: 0.5846\n",
      "Epoch 16/100\n",
      "448/448 [==============================] - 58s 129ms/step - loss: 1.1121 - accuracy: 0.5806 - val_loss: 1.0984 - val_accuracy: 0.5876\n",
      "Epoch 17/100\n",
      "448/448 [==============================] - 58s 129ms/step - loss: 1.0882 - accuracy: 0.5905 - val_loss: 1.0749 - val_accuracy: 0.6021\n",
      "Epoch 18/100\n",
      "448/448 [==============================] - 58s 129ms/step - loss: 1.0718 - accuracy: 0.5943 - val_loss: 1.0927 - val_accuracy: 0.5929\n",
      "Epoch 19/100\n",
      "448/448 [==============================] - 58s 130ms/step - loss: 1.0481 - accuracy: 0.6088 - val_loss: 1.0534 - val_accuracy: 0.6088\n",
      "Epoch 20/100\n",
      "448/448 [==============================] - 58s 130ms/step - loss: 1.0322 - accuracy: 0.6106 - val_loss: 1.0292 - val_accuracy: 0.6183\n",
      "Epoch 21/100\n",
      "448/448 [==============================] - 58s 130ms/step - loss: 1.0127 - accuracy: 0.6202 - val_loss: 1.0521 - val_accuracy: 0.6069\n",
      "Epoch 22/100\n",
      "448/448 [==============================] - 58s 130ms/step - loss: 0.9983 - accuracy: 0.6264 - val_loss: 1.0864 - val_accuracy: 0.6105\n",
      "Epoch 23/100\n",
      "448/448 [==============================] - 58s 130ms/step - loss: 0.9806 - accuracy: 0.6304 - val_loss: 1.0269 - val_accuracy: 0.6289\n",
      "Epoch 24/100\n",
      "448/448 [==============================] - 58s 130ms/step - loss: 0.9731 - accuracy: 0.6376 - val_loss: 1.0191 - val_accuracy: 0.6255\n",
      "Epoch 25/100\n",
      "448/448 [==============================] - 58s 130ms/step - loss: 0.9535 - accuracy: 0.6458 - val_loss: 0.9949 - val_accuracy: 0.6211\n",
      "Epoch 26/100\n",
      "448/448 [==============================] - 58s 130ms/step - loss: 0.9363 - accuracy: 0.6508 - val_loss: 1.0479 - val_accuracy: 0.6080\n",
      "Epoch 27/100\n",
      "448/448 [==============================] - 58s 130ms/step - loss: 0.9258 - accuracy: 0.6534 - val_loss: 1.0028 - val_accuracy: 0.6328\n",
      "Epoch 28/100\n",
      "448/448 [==============================] - 58s 130ms/step - loss: 0.9150 - accuracy: 0.6561 - val_loss: 1.0285 - val_accuracy: 0.6258\n",
      "Epoch 29/100\n",
      "448/448 [==============================] - 58s 130ms/step - loss: 0.8997 - accuracy: 0.6644 - val_loss: 1.0236 - val_accuracy: 0.6300\n",
      "Epoch 30/100\n",
      "448/448 [==============================] - 58s 130ms/step - loss: 0.8880 - accuracy: 0.6694 - val_loss: 0.9685 - val_accuracy: 0.6498\n",
      "Epoch 31/100\n",
      "448/448 [==============================] - 58s 130ms/step - loss: 0.8760 - accuracy: 0.6727 - val_loss: 1.0038 - val_accuracy: 0.6333\n",
      "Epoch 32/100\n",
      "448/448 [==============================] - 58s 130ms/step - loss: 0.8682 - accuracy: 0.6773 - val_loss: 0.9557 - val_accuracy: 0.6512\n",
      "Epoch 33/100\n",
      "448/448 [==============================] - 59s 131ms/step - loss: 0.8488 - accuracy: 0.6824 - val_loss: 1.0098 - val_accuracy: 0.6356\n",
      "Epoch 34/100\n",
      "448/448 [==============================] - 58s 129ms/step - loss: 0.8362 - accuracy: 0.6872 - val_loss: 0.9803 - val_accuracy: 0.6447\n",
      "Epoch 35/100\n",
      "448/448 [==============================] - 58s 129ms/step - loss: 0.8306 - accuracy: 0.6867 - val_loss: 0.9730 - val_accuracy: 0.6481\n",
      "Epoch 36/100\n",
      "448/448 [==============================] - 58s 129ms/step - loss: 0.8157 - accuracy: 0.6937 - val_loss: 0.9742 - val_accuracy: 0.6517\n",
      "Epoch 37/100\n",
      "448/448 [==============================] - 58s 129ms/step - loss: 0.8077 - accuracy: 0.6990 - val_loss: 0.9878 - val_accuracy: 0.6453\n",
      "Epoch 38/100\n",
      "448/448 [==============================] - 58s 130ms/step - loss: 0.7949 - accuracy: 0.7041 - val_loss: 0.9511 - val_accuracy: 0.6578\n",
      "Epoch 39/100\n",
      "448/448 [==============================] - 58s 129ms/step - loss: 0.7814 - accuracy: 0.7084 - val_loss: 0.9753 - val_accuracy: 0.6500\n",
      "Epoch 40/100\n",
      "448/448 [==============================] - 58s 130ms/step - loss: 0.7735 - accuracy: 0.7124 - val_loss: 0.9901 - val_accuracy: 0.6534\n",
      "Epoch 41/100\n",
      "448/448 [==============================] - 58s 130ms/step - loss: 0.7593 - accuracy: 0.7162 - val_loss: 0.9552 - val_accuracy: 0.6654\n",
      "Epoch 42/100\n",
      "448/448 [==============================] - 58s 130ms/step - loss: 0.7532 - accuracy: 0.7202 - val_loss: 0.9873 - val_accuracy: 0.6637\n",
      "Epoch 43/100\n",
      "448/448 [==============================] - 58s 129ms/step - loss: 0.7460 - accuracy: 0.7226 - val_loss: 0.9738 - val_accuracy: 0.6534\n",
      "Epoch 44/100\n",
      "448/448 [==============================] - 58s 129ms/step - loss: 0.7267 - accuracy: 0.7267 - val_loss: 0.9616 - val_accuracy: 0.6740\n",
      "Epoch 45/100\n",
      "448/448 [==============================] - 58s 129ms/step - loss: 0.7236 - accuracy: 0.7303 - val_loss: 0.9639 - val_accuracy: 0.6695\n",
      "Epoch 46/100\n",
      "448/448 [==============================] - 58s 129ms/step - loss: 0.7078 - accuracy: 0.7370 - val_loss: 0.9622 - val_accuracy: 0.6718\n",
      "Epoch 47/100\n",
      "448/448 [==============================] - 58s 129ms/step - loss: 0.6989 - accuracy: 0.7415 - val_loss: 0.9587 - val_accuracy: 0.6729\n",
      "Epoch 48/100\n",
      "448/448 [==============================] - 58s 129ms/step - loss: 0.6866 - accuracy: 0.7439 - val_loss: 0.9475 - val_accuracy: 0.6707\n",
      "Epoch 49/100\n",
      "448/448 [==============================] - 58s 129ms/step - loss: 0.6742 - accuracy: 0.7464 - val_loss: 1.0575 - val_accuracy: 0.6520\n",
      "Epoch 50/100\n",
      "448/448 [==============================] - 58s 129ms/step - loss: 0.6693 - accuracy: 0.7507 - val_loss: 0.9994 - val_accuracy: 0.6746\n",
      "Epoch 51/100\n",
      "448/448 [==============================] - 58s 130ms/step - loss: 0.6535 - accuracy: 0.7577 - val_loss: 0.9947 - val_accuracy: 0.6693\n",
      "Epoch 52/100\n",
      "448/448 [==============================] - 58s 130ms/step - loss: 0.6521 - accuracy: 0.7582 - val_loss: 0.9981 - val_accuracy: 0.6718\n",
      "Epoch 53/100\n",
      "448/448 [==============================] - 58s 130ms/step - loss: 0.6453 - accuracy: 0.7598 - val_loss: 1.0594 - val_accuracy: 0.6553\n",
      "Epoch 54/100\n",
      "448/448 [==============================] - 58s 129ms/step - loss: 0.6253 - accuracy: 0.7675 - val_loss: 0.9977 - val_accuracy: 0.6737\n",
      "Epoch 55/100\n",
      "448/448 [==============================] - 58s 130ms/step - loss: 0.6163 - accuracy: 0.7698 - val_loss: 1.0080 - val_accuracy: 0.6785\n",
      "Epoch 56/100\n",
      "448/448 [==============================] - 58s 129ms/step - loss: 0.6096 - accuracy: 0.7740 - val_loss: 1.0011 - val_accuracy: 0.6754\n",
      "Epoch 57/100\n",
      "448/448 [==============================] - 58s 130ms/step - loss: 0.5961 - accuracy: 0.7779 - val_loss: 0.9921 - val_accuracy: 0.6793\n",
      "Epoch 58/100\n",
      "448/448 [==============================] - 58s 130ms/step - loss: 0.5942 - accuracy: 0.7788 - val_loss: 1.0685 - val_accuracy: 0.6687\n",
      "Epoch 59/100\n",
      "448/448 [==============================] - 58s 129ms/step - loss: 0.5844 - accuracy: 0.7788 - val_loss: 1.0011 - val_accuracy: 0.6690\n",
      "Epoch 60/100\n",
      "448/448 [==============================] - 58s 130ms/step - loss: 0.5675 - accuracy: 0.7892 - val_loss: 1.0408 - val_accuracy: 0.6793\n",
      "Epoch 61/100\n",
      "448/448 [==============================] - 58s 130ms/step - loss: 0.5632 - accuracy: 0.7925 - val_loss: 1.0340 - val_accuracy: 0.6812\n",
      "Epoch 62/100\n",
      "448/448 [==============================] - 58s 130ms/step - loss: 0.5538 - accuracy: 0.7932 - val_loss: 1.1022 - val_accuracy: 0.6654\n",
      "Epoch 63/100\n",
      "448/448 [==============================] - 58s 130ms/step - loss: 0.5411 - accuracy: 0.7986 - val_loss: 1.0493 - val_accuracy: 0.6779\n",
      "Epoch 64/100\n",
      "448/448 [==============================] - 59s 131ms/step - loss: 0.5414 - accuracy: 0.7992 - val_loss: 1.0389 - val_accuracy: 0.6779\n",
      "Epoch 65/100\n",
      "448/448 [==============================] - 58s 130ms/step - loss: 0.5217 - accuracy: 0.8081 - val_loss: 1.0458 - val_accuracy: 0.6860\n",
      "Epoch 66/100\n",
      "448/448 [==============================] - 59s 131ms/step - loss: 0.5136 - accuracy: 0.8108 - val_loss: 1.0864 - val_accuracy: 0.6790\n",
      "Epoch 67/100\n",
      "448/448 [==============================] - 59s 131ms/step - loss: 0.5121 - accuracy: 0.8088 - val_loss: 1.0506 - val_accuracy: 0.6871\n",
      "Epoch 68/100\n",
      "448/448 [==============================] - 58s 130ms/step - loss: 0.5061 - accuracy: 0.8093 - val_loss: 1.0904 - val_accuracy: 0.6801\n",
      "Epoch 69/100\n",
      "448/448 [==============================] - 59s 131ms/step - loss: 0.4914 - accuracy: 0.8180 - val_loss: 1.1184 - val_accuracy: 0.6751\n",
      "Epoch 70/100\n",
      "448/448 [==============================] - 59s 131ms/step - loss: 0.4894 - accuracy: 0.8174 - val_loss: 1.1253 - val_accuracy: 0.6854\n",
      "Epoch 71/100\n",
      "448/448 [==============================] - 59s 131ms/step - loss: 0.4826 - accuracy: 0.8221 - val_loss: 1.1054 - val_accuracy: 0.6810\n",
      "Epoch 72/100\n",
      "448/448 [==============================] - 59s 130ms/step - loss: 0.4722 - accuracy: 0.8248 - val_loss: 1.1377 - val_accuracy: 0.6721\n",
      "Epoch 73/100\n",
      "448/448 [==============================] - 58s 130ms/step - loss: 0.4663 - accuracy: 0.8247 - val_loss: 1.1235 - val_accuracy: 0.6785\n",
      "Epoch 74/100\n",
      "448/448 [==============================] - 59s 131ms/step - loss: 0.4565 - accuracy: 0.8312 - val_loss: 1.1618 - val_accuracy: 0.6810\n",
      "Epoch 75/100\n",
      "448/448 [==============================] - 59s 131ms/step - loss: 0.4562 - accuracy: 0.8315 - val_loss: 1.1622 - val_accuracy: 0.6843\n",
      "Epoch 76/100\n",
      "448/448 [==============================] - 59s 131ms/step - loss: 0.4407 - accuracy: 0.8375 - val_loss: 1.1181 - val_accuracy: 0.6840\n",
      "Epoch 77/100\n",
      "448/448 [==============================] - 59s 131ms/step - loss: 0.4325 - accuracy: 0.8386 - val_loss: 1.1192 - val_accuracy: 0.6863\n",
      "Epoch 78/100\n",
      "448/448 [==============================] - 59s 131ms/step - loss: 0.4289 - accuracy: 0.8417 - val_loss: 1.1870 - val_accuracy: 0.6874\n",
      "Epoch 79/100\n",
      "448/448 [==============================] - 59s 131ms/step - loss: 0.4184 - accuracy: 0.8470 - val_loss: 1.1280 - val_accuracy: 0.6785\n",
      "Epoch 80/100\n",
      "448/448 [==============================] - 59s 131ms/step - loss: 0.4140 - accuracy: 0.8471 - val_loss: 1.1567 - val_accuracy: 0.6868\n",
      "Epoch 81/100\n",
      "448/448 [==============================] - 59s 131ms/step - loss: 0.4152 - accuracy: 0.8476 - val_loss: 1.1431 - val_accuracy: 0.6865\n",
      "Epoch 82/100\n",
      "448/448 [==============================] - 59s 131ms/step - loss: 0.4023 - accuracy: 0.8519 - val_loss: 1.1604 - val_accuracy: 0.6810\n",
      "Epoch 83/100\n",
      "448/448 [==============================] - 59s 131ms/step - loss: 0.3906 - accuracy: 0.8570 - val_loss: 1.1999 - val_accuracy: 0.6818\n",
      "Epoch 84/100\n",
      "448/448 [==============================] - 59s 132ms/step - loss: 0.3904 - accuracy: 0.8584 - val_loss: 1.1710 - val_accuracy: 0.6890\n",
      "Epoch 85/100\n",
      "448/448 [==============================] - 59s 131ms/step - loss: 0.3835 - accuracy: 0.8587 - val_loss: 1.2584 - val_accuracy: 0.6829\n",
      "Epoch 86/100\n",
      "448/448 [==============================] - 58s 130ms/step - loss: 0.3772 - accuracy: 0.8617 - val_loss: 1.2231 - val_accuracy: 0.6812\n",
      "Epoch 87/100\n",
      "448/448 [==============================] - 58s 130ms/step - loss: 0.3776 - accuracy: 0.8615 - val_loss: 1.2119 - val_accuracy: 0.6896\n",
      "Epoch 88/100\n",
      "448/448 [==============================] - 58s 130ms/step - loss: 0.3653 - accuracy: 0.8654 - val_loss: 1.2634 - val_accuracy: 0.6796\n",
      "Epoch 89/100\n",
      "448/448 [==============================] - 58s 130ms/step - loss: 0.3652 - accuracy: 0.8654 - val_loss: 1.2868 - val_accuracy: 0.6732\n",
      "Epoch 90/100\n",
      "448/448 [==============================] - 58s 130ms/step - loss: 0.3496 - accuracy: 0.8719 - val_loss: 1.2906 - val_accuracy: 0.6698\n",
      "Epoch 91/100\n",
      "448/448 [==============================] - 58s 130ms/step - loss: 0.3520 - accuracy: 0.8687 - val_loss: 1.2126 - val_accuracy: 0.6874\n",
      "Epoch 92/100\n",
      "448/448 [==============================] - 58s 130ms/step - loss: 0.3451 - accuracy: 0.8727 - val_loss: 1.2829 - val_accuracy: 0.6654\n",
      "Epoch 93/100\n",
      "448/448 [==============================] - 58s 130ms/step - loss: 0.3465 - accuracy: 0.8715 - val_loss: 1.2488 - val_accuracy: 0.6846\n",
      "Epoch 94/100\n",
      "448/448 [==============================] - 58s 130ms/step - loss: 0.3340 - accuracy: 0.8768 - val_loss: 1.3181 - val_accuracy: 0.6810\n",
      "Epoch 95/100\n",
      "448/448 [==============================] - 58s 130ms/step - loss: 0.3286 - accuracy: 0.8785 - val_loss: 1.3068 - val_accuracy: 0.6826\n",
      "Epoch 96/100\n",
      "448/448 [==============================] - 58s 129ms/step - loss: 0.3278 - accuracy: 0.8789 - val_loss: 1.3691 - val_accuracy: 0.6734\n",
      "Epoch 97/100\n",
      "448/448 [==============================] - 58s 129ms/step - loss: 0.3207 - accuracy: 0.8810 - val_loss: 1.2827 - val_accuracy: 0.6776\n",
      "Epoch 98/100\n",
      "448/448 [==============================] - 58s 129ms/step - loss: 0.3165 - accuracy: 0.8828 - val_loss: 1.3481 - val_accuracy: 0.6673\n",
      "Epoch 99/100\n",
      "448/448 [==============================] - 58s 129ms/step - loss: 0.3073 - accuracy: 0.8895 - val_loss: 1.3876 - val_accuracy: 0.6773\n",
      "Epoch 100/100\n",
      "448/448 [==============================] - 58s 129ms/step - loss: 0.3024 - accuracy: 0.8899 - val_loss: 1.4114 - val_accuracy: 0.6737\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100  \n",
    "history = model.fit(train_flow, \n",
    "                    steps_per_epoch=len(X_train) / batch_size, \n",
    "                    epochs=num_epochs,  \n",
    "                    # verbose=1,  \n",
    "                    validation_data=test_flow,\n",
    "                    validation_steps=len(X_test) / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# creating the weights and the json file\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the weights and the json file\n",
    "from tensorflow.keras.models import model_from_json\n",
    "model = model_from_json(open(\"model.json\", \"r\").read())\n",
    "model.load_weights('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "face_haar_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20548/3456853110.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVideoCapture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[0mcap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misOpened\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mchannel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m#---------------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "cap=cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    res,frame=cap.read()\n",
    "    height, width , channel = frame.shape\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Creating an Overlay window to write prediction and cofidencesub_img = frame[0:int(height/6),0:int(width)]black_rect = np.ones(sub_img.shape, dtype=np.uint8)*0\n",
    "    res = cv2.addWeighted(sub_img, 0.77, black_rect,0.23, 0)\n",
    "    FONT = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    FONT_SCALE = 0.8\n",
    "    FONT_THICKNESS = 2\n",
    "    lable_color = (10, 10, 255)\n",
    "    lable = \"Detector\"\n",
    "    lable_dimension = cv2.getTextSize(lable,FONT ,FONT_SCALE,FONT_THICKNESS)[0]\n",
    "    textX = int((res.shape[1] - lable_dimension[0]) / 2)\n",
    "    textY = int((res.shape[0] + lable_dimension[1]) / 2)\n",
    "    cv2.putText(res, lable, (textX,textY), FONT, FONT_SCALE, (0,0,0), FONT_THICKNESS)\n",
    "    # prediction part --------------------------------------------------------------------------gray_image= cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_haar_cascade.detectMultiScale(gray_image )\n",
    "    try:\n",
    "        for (x,y, w, h) in faces:\n",
    "            cv2.rectangle(frame, pt1 = (x,y),pt2 = (x+w, y+h), color = (255,0,0),thickness =  2)\n",
    "            roi_gray = gray_image[y-5:y+h+5,x-5:x+w+5]\n",
    "            roi_gray=cv2.resize(roi_gray,(48,48))\n",
    "            image_pixels = img_to_array(roi_gray)\n",
    "            image_pixels = np.expand_dims(image_pixels, axis = 0)\n",
    "            image_pixels /= 255\n",
    "            predictions = model.predict(image_pixels)\n",
    "            max_index = np.argmax(predictions[0])\n",
    "            emotion_detection = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')\n",
    "            emotion_prediction = emotion_detection[max_index]\n",
    "            cv2.putText(res, \"Sentiment: {}\".format(emotion_prediction), (0,textY+22+5), FONT,0.7, lable_color,2)\n",
    "            lable_violation = 'Confidence: {}'.format(str(np.round(np.max(predictions[0])*100,1))+ \"%\")\n",
    "            violation_text_dimension = cv2.getTextSize(lable_violation,FONT,FONT_SCALE,FONT_THICKNESS )[0]\n",
    "            violation_x_axis = int(res.shape[1]- violation_text_dimension[0])\n",
    "            cv2.putText(res, lable_violation, (violation_x_axis,textY+22+5), FONT,0.7, lable_color,2)\n",
    "    except :\n",
    "        pass\n",
    "    frame[0:int(height/6),0:int(width)] = res\n",
    "    cv2.imshow('frame', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        breakcap.release()\n",
    "cv2.destroyAllWindows"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4f3b078249380ea762697f4f8f6aea77b3d6e43cbb1b18cbb73d8cde5aa597e7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
